{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec \n",
    "Presentation by: Lauren Funston\n",
    "\n",
    "## What is word2vec? \n",
    "word2vec is an open-source project created by Google in 2013 that takes a corpus (large body of text) and converts the words into vector representations, based on the context in which they appear. This allows \"clusters\" of similar words to be found (for example, the word \"piano\" is similar to other instruments, such as \"guitar\", \"violin\", etc.) This has a number of applications, such as text prediction, translation, and analysis. The image below shows an example of translation: models can be created for each language, and the clusters formed will likely be similar between languages, allowing for translation based on cluster locations. \n",
    "\n",
    "![translation using word2vec](img/translation.png)\n",
    "Source: [A Beginner's Guide to Word2Vec and Neural Word Embeddings (pathmind.com)](https://wiki.pathmind.com/word2vec)\n",
    "\n",
    "## How does it work? \n",
    "word2vec goes through a corpus document word-by-word, updating the vector for each word as it goes based on how reliably it is able to predict the words surrounding that word. The context window is a parameter for the algorithm, usually set to around 10 words (as recommended by the original authors). The resulting vectors are usually in 100+ dimensional space (the number of dimensions is also a parameter).\n",
    "\n",
    "Once vectors have been created, they are compared using *cosine similarity*. Essentially, the cosine of the angle between two vectors is taken. A value closer to 0 indicates that words are less similar/have no relationship, and the cosine of two orthogonal vectors is 0 ($\\cos(\\frac{\\pi}{2}) = 0$). A value closer to 1 indicates that words are more similar, and the cosine of two identical vectors is 1 ($\\cos(0) = 1$). A value close to -1 indicates that words have the opposite meaning, and the cosine of two opposite vectors is -1 ($\\cos(\\pi) = -1$)\n",
    "\n",
    "The cosine between two vectors can be derived from their inner product:\n",
    "$$ A \\cdot B = \\lvert \\lvert A \\rvert \\rvert \\lvert \\lvert B \\rvert \\rvert \\cos(\\theta)$$\n",
    "$$ \\cos(\\theta) = \\frac{A \\cdot B}{\\lvert \\lvert A \\rvert \\rvert \\lvert \\lvert B \\rvert \\rvert}$$\n",
    "Thus the complete equation for computing the cosine similarity of two vectors is as follows: \n",
    "$$ \\cos(\\theta) = \\frac{\\sum_{i=1}^n A_i \\times B_i}{\\sqrt{\\sum_{i=1}^n A_i^2} \\times \\sqrt{\\sum_{i=1}^n B_i^2}}$$\n",
    "\n",
    "![cosine similarity diagrams](img/cosine-similarity-vectors.original.jpg)\n",
    "Source: [Cosine Similarity (datasci.com)](https://www.learndatasci.com/glossary/cosine-similarity/)\n",
    "\n",
    "## Examples\n",
    "\n",
    "We need to start by training the model (aka, create the vectors) using a corpus. I've included in this repository the one linked in the [documentation](https://nbviewer.org/github/danielfrg/word2vec/blob/main/examples/word2vec.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: word2vec -train example/data/text8 -output example/data/text8.bin -size 100 -window 5 -sample 1e-3 -hs 0 -negative 5 -threads 12 -iter 5 -min-count 5 -alpha 0.025 -debug 2 -binary 1 -cbow 1\n",
      "Starting training using file example/data/text8\n",
      "Vocab size: 71291\n",
      "Words in train file: 16718843\n",
      "Alpha: 0.000002  Progress: 100.04%  Words/thread/sec: 403.19k  c: 353.01k  6.52k  ec: 384.80k  : 0.024718  Progress: 1.14%  Words/thread/sec: 373.64k  667  Progress: 1.34%  Words/thread/sec: 391.09k  ogress: 1.55%  Words/thread/sec: 385.46k  read/sec: 393.34k  ha: 0.024308  Progress: 2.78%  Words/thread/sec: 393.18k   3.40%  Words/thread/sec: 399.79k   ec: 400.35k  .44k   Progress: 4.85%  Words/thread/sec: 398.84k  /thread/sec: 399.81k  ss: 6.70%  Words/thread/sec: 401.51k  Words/thread/sec: 401.07k  02.45k  k  d/sec: 402.04k  61%  Words/thread/sec: 403.08k  rds/thread/sec: 403.23k  022589  Progress: 9.66%  Words/thread/sec: 401.11k  3  Progress: 10.28%  Words/thread/sec: 401.04k  : 400.61k  ec: 402.57k  /thread/sec: 400.09k  Words/thread/sec: 401.00k   0.022051  Progress: 11.81%  Words/thread/sec: 401.64k  ead/sec: 401.63k  ogress: 13.02%  Words/thread/sec: 401.67k  72k  /thread/sec: 402.29k  ead/sec: 401.61k  23%  Words/thread/sec: 401.54k  .63k  : 401.47k   15.85%  Words/thread/sec: 402.26k  lpha: 0.020886  Progress: 16.47%  Words/thread/sec: 402.12k  s/thread/sec: 401.64k  .27%  Words/thread/sec: 401.68k  ress: 17.48%  Words/thread/sec: 401.28k  583  Progress: 17.68%  Words/thread/sec: 401.91k  c: 401.98k  ad/sec: 401.56k  sec: 402.17k  0.020051  Progress: 19.82%  Words/thread/sec: 401.80k  read/sec: 401.81k  830  Progress: 20.69%  Words/thread/sec: 401.41k  ha: 0.019748  Progress: 21.02%  Words/thread/sec: 400.78k  read/sec: 401.66k  /thread/sec: 401.33k  19389  Progress: 22.46%  Words/thread/sec: 401.72k  : 23.02%  Words/thread/sec: 401.24k  Progress: 24.01%  Words/thread/sec: 402.15k  33k  Progress: 24.35%  Words/thread/sec: 402.15k  /sec: 402.81k    Words/thread/sec: 402.57k  018644  Progress: 25.44%  Words/thread/sec: 402.25k  read/sec: 402.03k  rogress: 26.69%  Words/thread/sec: 401.68k  .05k  /sec: 400.79k  ds/thread/sec: 400.94k  26  Progress: 28.31%  Words/thread/sec: 401.07k  : 401.13k   29.52%  Words/thread/sec: 400.89k  lpha: 0.017471  Progress: 30.13%  Words/thread/sec: 401.02k  s: 30.64%  Words/thread/sec: 401.59k  ogress: 30.94%  Words/thread/sec: 401.97k  gress: 31.14%  Words/thread/sec: 401.29k  1k    Words/thread/sec: 401.41k  1.92k  01.93k  .97%  Words/thread/sec: 401.79k  .73k  ogress: 34.17%  Words/thread/sec: 401.92k  16409  Progress: 34.38%  Words/thread/sec: 402.24k  read/sec: 401.75k  76k  sec: 401.53k  : 0.015923  Progress: 36.32%  Words/thread/sec: 401.85k  3%  Words/thread/sec: 401.85k  s: 37.06%  Words/thread/sec: 401.65k  : 37.33%  Words/thread/sec: 402.05k  : 38.15%  Words/thread/sec: 401.95k  rogress: 38.64%  Words/thread/sec: 402.07k  9  Progress: 39.14%  Words/thread/sec: 401.75k   Words/thread/sec: 402.09k  014864  Progress: 40.56%  Words/thread/sec: 402.36k  02.54k  .56%  Words/thread/sec: 402.30k  a: 0.014459  Progress: 42.18%  Words/thread/sec: 401.84k  hread/sec: 401.86k  Progress: 43.39%  Words/thread/sec: 401.81k  /sec: 402.04k  ad/sec: 401.67k  gress: 44.81%  Words/thread/sec: 401.58k  6k    Words/thread/sec: 401.44k  .013343  Progress: 46.64%  Words/thread/sec: 401.23k  d/sec: 401.32k  rds/thread/sec: 401.28k  7.65%  Words/thread/sec: 400.99k  ha: 0.012939  Progress: 48.26%  Words/thread/sec: 400.90k  : 401.01k  6k  0.012403  Progress: 50.40%  Words/thread/sec: 400.23k  ec: 400.37k  2231  Progress: 51.09%  Words/thread/sec: 400.56k  012085  Progress: 51.67%  Words/thread/sec: 400.39k  : 400.33k  6  Progress: 52.67%  Words/thread/sec: 400.40k  Progress: 53.13%  Words/thread/sec: 400.40k   Words/thread/sec: 400.70k  00.71k  a: 0.011423  Progress: 54.32%  Words/thread/sec: 400.77k  Progress: 54.36%  Words/thread/sec: 400.92k  .48%  Words/thread/sec: 400.82k  ss: 54.56%  Words/thread/sec: 401.14k    Progress: 55.01%  Words/thread/sec: 400.89k  38%  Words/thread/sec: 400.84k  c: 400.92k  : 56.75%  Words/thread/sec: 400.89k  1.17k  01.40k  .18%  Words/thread/sec: 401.28k  a: 0.010306  Progress: 58.79%  Words/thread/sec: 401.33k  hread/sec: 401.33k  Progress: 60.06%  Words/thread/sec: 401.45k  1.53k  d/sec: 401.40k  rds/thread/sec: 401.59k  5k  ss: 62.70%  Words/thread/sec: 401.41k   Progress: 63.12%  Words/thread/sec: 401.56k   Progress: 63.69%  Words/thread/sec: 401.45k  pha: 0.009000  Progress: 64.03%  Words/thread/sec: 401.60k  c: 401.63k  64.33%  Words/thread/sec: 401.59k  rogress: 64.54%  Words/thread/sec: 401.66k  6  Progress: 65.11%  Words/thread/sec: 401.59k  s/thread/sec: 401.68k  8504  Progress: 66.00%  Words/thread/sec: 401.86k  pha: 0.008370  Progress: 66.53%  Words/thread/sec: 401.75k  ress: 67.44%  Words/thread/sec: 401.97k  ogress: 67.58%  Words/thread/sec: 401.89k  008060  Progress: 67.77%  Words/thread/sec: 401.93k  0.008057  Progress: 67.78%  Words/thread/sec: 402.01k  8k  ogress: 68.71%  Words/thread/sec: 401.92k  rds/thread/sec: 401.73k  : 0.007577  Progress: 69.71%  Words/thread/sec: 401.94k  0.007337  Progress: 70.67%  Words/thread/sec: 401.95k   0.007298  Progress: 70.82%  Words/thread/sec: 401.86k    Words/thread/sec: 401.96k  06945  Progress: 72.23%  Words/thread/sec: 401.81k  sec: 401.90k  ss: 73.44%  Words/thread/sec: 401.95k  9  Progress: 73.98%  Words/thread/sec: 402.09k   402.20k  74.86%  Words/thread/sec: 402.16k  pha: 0.006136  Progress: 75.47%  Words/thread/sec: 402.16k  /thread/sec: 402.19k  .67%  Words/thread/sec: 402.06k   0%  Words/thread/sec: 402.13k  ss: 77.56%  Words/thread/sec: 402.29k  : 402.12k    Words/thread/sec: 402.22k  .005425  Progress: 78.31%  Words/thread/sec: 402.21k  d/sec: 402.17k  .49%  Words/thread/sec: 402.21k  .004982  Progress: 80.08%  Words/thread/sec: 402.42k  02.44k  s: 81.04%  Words/thread/sec: 402.26k  ords/thread/sec: 402.43k    Words/thread/sec: 402.49k  /thread/sec: 402.46k   Progress: 83.56%  Words/thread/sec: 402.49k  s: 84.05%  Words/thread/sec: 402.59k  0.003880  Progress: 84.49%  Words/thread/sec: 402.35k    ords/thread/sec: 402.61k  3525  Progress: 85.91%  Words/thread/sec: 402.57k  ec: 402.57k  s: 87.13%  Words/thread/sec: 402.66k  ad/sec: 402.70k  8.54%  Words/thread/sec: 402.73k  0.002722  Progress: 89.13%  Words/thread/sec: 402.65k  ec: 402.74k  sec: 402.77k  s/thread/sec: 402.78k  thread/sec: 402.65k  /sec: 402.71k  0.002064  Progress: 91.76%  Words/thread/sec: 402.73k  ec: 402.82k  sec: 402.73k  d/sec: 402.75k  .57%  Words/thread/sec: 402.78k  hread/sec: 402.80k  ec: 402.77k  : 402.71k  .001198  Progress: 95.22%  Words/thread/sec: 402.84k  086  Progress: 95.67%  Words/thread/sec: 402.96k  Words/thread/sec: 402.92k    lpha: 0.000920  Progress: 96.33%  Words/thread/sec: 402.70k  8k  thread/sec: 402.79k  s: 97.65%  Words/thread/sec: 402.71k  72  Progress: 98.13%  Words/thread/sec: 402.73k  ad/sec: 402.88k  9.10%  Words/thread/sec: 402.81k  ha: 0.000062  Progress: 99.76%  Words/thread/sec: 403.08k  "
     ]
    }
   ],
   "source": [
    "import word2vec\n",
    "# size here refers to dimensionality of vectors\n",
    "word2vec.word2vec('example/data/text8', 'example/data/text8.bin', size=100, binary=True, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained the model, we can view the vectors that were generated: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71291, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = word2vec.load('example/data/text8.bin')\n",
    "model.vectors.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the dimensions of the array: there are 71291 words, and each is represented by a dimension 100 vector. \n",
    "\n",
    "We can now use the model to find similar words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['violin', 'cello', 'sonatas', 'harpsichord', 'sonata', 'concertos',\n",
       "       'concerto', 'orchestral', 'orchestra', 'quartet'], dtype='<U78')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes, metrics = model.similar(\"piano\")\n",
    "model.vocab[indexes]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model returns words that are instruments and/or have to do with music. So far so good! \n",
    "\n",
    "Let's see an example of using an analogy. The analogy I'll be using here is \"Beijing - China + Russia\", and the answer I'm expecting is \"Moscow\" (I know that Beijing is the capital of China, but I want to know the capital of Russia):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['moscow', 'helsinki', 'riga', 'lisbon', 'transylvania', 'brussels',\n",
       "       'barcelona', 'munich', 'budapest', 'switzerland'], dtype='<U78')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes, metrics = model.analogy(pos=[\"beijing\", \"russia\"], neg=[\"china\"])\n",
    "model.vocab[indexes]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, it got \"Moscow\" as the first guess, with various other European cities as the next most likely choices. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "I'm curious about the appropriate size of a corpus in order to properly train the model. The corpus included here (and linked in the documentation) is quite large, but seems to work quite well. I wonder if an even larger corpus would increase the accuracy of the model, or if it would significantly slow it down; similarly, if a smaller corpus would be able to be just as accurate and/or quicker. \n",
    "\n",
    "## Proposed Experiment\n",
    "\n",
    "I think the dimensionality of the vectors is an interesting aspect of the algorithm, and ties back to the idea of bias vs. variance that we've been discussing in class. It seems that the 100-dimension vectors used in the example is on the lower end of dimensions used for this algorithms (most examples I've seen have used 500+). My experiment would be to see how the \"accuracy\" of the algorithm improves (or declines) as more dimensions are added to the vectors. I imagine that adding more dimensions would helpful for a bit to create a better \"fit\" for the model as it would be able to provide more information about each word (thus reducing bias), but I also think that after a certain point, too many dimensions would slow down the algorithm and introduce too much noise to the results (increasing variance). \n",
    "\n",
    "My proposed experiment would have a set of proposed analogies with an expected \"correct\" answer, and measure how well the algorithm is able to get the correct answer based on how many dimensions are included in the vectors. So dimensionality would be on the x axis, ranging from 100 to, say 10000 (not sure if that's computationally realistic) and on the y axis would be accuracy. \n",
    "\n",
    "As a closing example, here's the 100-dimension model failing to answer a similar analogy accurately (Rome - Italy + America), which should ideally return a value like \"Washington D.C.\" (the capital of the US). Would increasing the dimensionality of the model improve the guesses? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mormon', 'carolina', 'men', 'nicaea', 'lincoln', 'women', 'bible',\n",
       "       'prayer', 'chalcedon', 'dedication'], dtype='<U78')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes, metrics = model.analogy(pos=[\"rome\", \"america\"], neg=[\"italy\"])\n",
    "model.vocab[indexes]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "528ef8bef0bea82cd0ba4f022aea96994542db39a47a3a8c787d7c0de1b2affe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
